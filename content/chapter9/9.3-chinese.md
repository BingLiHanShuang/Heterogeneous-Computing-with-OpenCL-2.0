#9.3 OpenCL实现

本节，我们将了解一下如何在GPU上实现直方图构建。我们首先实现实现了一个常规内核，该内核代码基于串行和OpenMP版本的算法。然后，我们在来对GPU实现进行优化，这里使用到的技巧是使用合并访问的方式，以及通过局部内存的方式。

##9.3.1 常规GPU实现：GPU1

根据清单9.2中的OpenMP实现，我们可以实现一个最简单OpenCL版本，也就是将循环迭代拆开，让每个工作项完成一个标识符的归属计算。

不过，在OpenMP实现时，我们遇到了条件竞争的问题，这里工作项需要对在全局内存上的直方图进行更新。为了解决这一问题，在OpenCL中我们依旧使用原子操作。下面的OpenCL内核就是我们的实现。我们暂且称这段代码为GPU1：

{%ace edit=false, lang='c_cpp'%}
__kernel
void kernelGPU1(
  __global float *descriptors,
  __global float *centroids,
  __global int *histogram,
  int n_descriptors,
  int n_centroids,
  int nfeatures){

  // Global ID identifies SURF descriptor
  int desc_id = get_global_id(0);
  
  int membership = 0;
  float min_dist = FLT_MAX;
  
  // For each cluster, compute the membership
  for (int j = 0; j < n_centroids; j++){
    float dist = 0;
    
    // n_features: No. of elements in each descriptor(64)
    // Calculate the distance between the descriptor and the centroid
    for (int k = 0; k < n_features; k++){
      float temp = descriptors[desc_id * n_features + k] -
        centroids[j * n_features + k];
      dist += temp * temp;
    }
    
    // Update the minimum distance
    if (dist < min_dist){
      membership = j;
    }
  }
  
  // Atomic increment of histogram bin
  atomic_fetch_add_explicit(&histogram[membership], 1, memory_order_relaxed, memory_scope_device);
}
{%endace%}

代码清单9.3 内核GPU1，直方图基线内核

注意清单9.3中，第37行的原子加操作，使用的自由序进行。我们选择这个内存序的原因是，因为这里的更新操作比较简单，并不需要对内存访问顺序进行严格要求。详细内容可回顾第7章中的相关章节。

##9.3.2 合并访问内存：GPU2

这个例子中对于数据的访问不存在跨距，所以工作项可以在GPU执行单指令多数据命令(SIMD)。并且SURF描述符和集群质心矢量都具有64个连续数据。再来看看清单9.3中的第24行，并且注意描述符访问的方式。对于给定的GPU硬件来说，第24行对于内存的访问是否高效呢？

假设有4个并行的工作项，其全局索引分别是0到3。在GPU执行最内部的循环时，这个四个工作项所访问的数据间具有很大的跨距——在这个版本的内核代码中，数据跨距为n_features。假设我们现在在处理质心0(j=0)，并且正在处理质心0与特征0(k=0)之间的距离。那么工作项分别访问的数据为：descriptors[0]、descriptors[64]、descriptors[128]、descriptors[192]。那么计算下一个特征时就要访问descriptors[1]、descriptors[65]、descriptors[129]、descriptors[193]，以此类推。

第8章中我们介绍了对于连续数据的合并访问方式，该方式只向内存系统发出更少的请求，以高效的方式获取相应的数据。不过，跨距访问方式需要产生多个访存请求，从而导致性能下降。这种带有跨距的访存方式是无法进行合并访问的。

为了通过使用合并访问的方式提升内存带宽的利用率，我们需要调整数据在descriptors存放的顺序。这个可以通过一种常用的矩阵操作来完成——转置，如图9.3所示。转置过程中，矩阵的行列坐标互换。我们可以创建一个简单的内核来完成这项工作。

![](../../images/chapter9/9-3.png)

图9.3 将描述符中的数据进行转置，以便进行合并访问

![](../../images/chapter9/9-4.png)

图9.4 对一维数组进行转置。

对一维数组的转置如图9.4所示。转置后，descriptors[0]、descriptors[64]、descriptors[128]、descriptors[192]就是连续的了。并且内核内部可以进行合并访问，只需要一个访存请求，即可直接将4个连续的数据取出。

{%ace edit=false, lang='c_cpp'%}
__kernel
void kernelGPU1(
  __global float *descriptors,
  __global float *centroids,
  __global int *histogram,
  int n_descriptors,
  int n_centroids,
  int nfeatures){

  // Global ID identifies SURF descriptor
  int desc_id = get_global_id(0);
  
  int membership = 0;
  float min_dist = FLT_MAX;
  
  // For each cluster, compute the membership
  for (int j = 0; j < n_centroids; j++){
    float dist = 0;
    
    // n_features: No. of elements in each descriptor(64)
    // Calculate the distance between the descriptor and the centroid
    for (int k = 0; k < n_features; k++){
      float temp = descriptors[k * n_descriptors + desc_id] -
        centroids[j * n_features + k];
      dist += temp * temp;
    }
    
    // Update the minimum distance
    if (dist < min_dist){
      membership = j;
    }
  }
  
  // Atomic increment of histogram bin
  atomic_fetch_add_explicit(&histogram[membership], 1, memory_order_relaxed, memory_scope_device);
}
{%endace%}

代码清单9.4 使用内存合并访问的内核——GPU2

清单9.4中的第24行，现在使用的是k * n_descriptors + desc_id，可与清单9.3进行对比。当k和n_descriptors具有相同的值时，所有工作项就挨个对数据进行计算，不同工作项可以通过唯一标识符对数据进行访问(desc_id)。我们之前举了4个工作组的例子，在k=0时，就只需要访问descriptors[0]、descriptors[1]、descriptors[2]和descriptors[3]即可。当k=1时，则需要访问descriptors[64]、descriptors[65]、descriptors[66]和descriptors[67]。这样的访存方式是最理想的，并且在GPU上进行合并访问可以高效的对内存系统进行操作。

##9.3.3 向量化计算：GPU3

##9.3.4 将SURF特征放入局部内存：GPU4

##9.3.5 将聚类中点坐标放入常量内存：GPU5